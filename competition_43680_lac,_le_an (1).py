# -*- coding: utf-8 -*-
"""Competition #43680 - LAC, LE AN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U8OS6Dvc7MaQLxCSv06ugU5mRpKVHssr

# **Leaders in Training Program (LTP) - Data Science Stream (P3) Written Assignment**

Competition #43680

LAC, LE AN (Leann)

February 18, 2025

This notebook (code and text) is used to answer the questions as per requirements of Competition #43680 Leaders in Training Program (LTP) - Data Science Stream (P3). The analysis task is completed using Python programming language along with statistical package(s). This Jupyter notebook is prepared on Google Colab with datasets are saved in Google Drive.

## **1. Connecting Google Drive and import libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# connect to Google Drive
from google.colab import drive
drive.mount('/content/drive')
# %cd "/content/drive/My Drive/LTP"

# list the folders and files in the connected folder
!ls

# install packages
!pip install sidetable

# import external Python libraries
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sidetable
from statsmodels.formula.api import ols

import warnings
warnings.filterwarnings('ignore')

"""## **2. Importing data files**"""

# import the building and safety inspections dataset
inspections_data = pd.read_csv('data/Building_and_Safety_Inspections_20250218.csv')
print(inspections_data.head())

print(inspections_data.shape)

"""Dimension of this dataset is 10,237,490 x 7."""

# import the building and safety permit dataset
permit_data = pd.read_csv('data/Building_and_Safety_Permit_Information_Old_20250218.csv')
print(permit_data.head())

print(permit_data.shape)

"""Dimension of this dataset is 1,635,148 x 59.

## **3. Data preparation**

To answer question 1, we may need to link the inspection_data with permit_data to have more information about permits. The key to link these two datasets are the permit number which is named as PERMIT and PCIS Permit # in inspection_data and permit_data, respectively. However, they were also stored in different formats. Thus, we first need to standardize the format before linking.
"""

# standardize the format
permit_data['PERMIT'] = permit_data['PCIS Permit #'].str.replace('-', ' ')

print(permit_data['PERMIT'])

"""Check if there are any applications linked between these two datasets."""

inspections_data['Check'] = inspections_data['PERMIT'].isin(permit_data['PERMIT'])

inspections_data['Check'].sum()

"""Due to the limitation in computing resource, instead of merging all information, a subset of information about permit informaiton will be extract in a new_permit_data."""

subset_permit_data = permit_data.loc[:,['PERMIT','Permit Type','Permit Sub-Type','Permit Category','Zip Code',
                                        'Contractor City','Contractor State',]]

subset_permit_data_CA = subset_permit_data[subset_permit_data['Contractor State']=='CA']

# merge data
permit_inspection_data = pd.merge(inspections_data, subset_permit_data_CA, on='PERMIT', how='inner')

# check dimension of merged data
print(permit_inspection_data.shape)

# drop the check column
permit_inspection_data.drop(columns=['Check'], inplace=True)

permit_inspection_data.head()

"""## **4. Question 1**
Make a table **and** a visualization showing an interesting characteristic of the permit and inspection dataset.

"""

# count NaN values under an entire DataFrame:
permit_inspection_data.isna().any()

# calculate percentage of missing values for each column
missing_percentage = (permit_inspection_data.isnull().mean() * 100).round(2)

# create a bar plot
plt.figure(figsize=(8, 6))
missing_percentage.plot(kind='bar', color='blue')

# add labels and title
plt.xlabel('Variables')
plt.ylabel('Percentage of missing values')
plt.xticks(rotation = 45)
plt.ylim(0,100)
plt.title('Percentage of missing values by variables')

# add percentage values on top of each bar
for i, val in enumerate(missing_percentage):
  plt.text(i, val, f'{val}%', ha='center', va='bottom')

# save the plot as a file
plt.savefig('figures/missing_percentage.png')

# show the plot
plt.tight_layout()
plt.show()

"""If we look at the missing percentage chart, contractor state has highest missing percentage, corresponding to 25.07%. Since th dataset is quite large, due to the nature of this assignment, we will not further discuss about imputation. Instead, also due to limitation of computing resources, we will show how to use Python to deleting missingness."""

permit_inspection_data_cleaned = permit_inspection_data.dropna()
print(permit_inspection_data_cleaned.shape)

"""After merging two datasets, deleting all rows with missing values, the size of the dataset 5,790,975 applications x 13 variables/features."""

# recheck missingness
permit_inspection_data_cleaned.isna().any()

"""No missing values found in the cleaned data.

### 4.1 Tables

**Permit Status**
"""

table_permit_status = permit_inspection_data_cleaned.stb.freq(['Permit Status'])[['Permit Status','count','percent']]
table_permit_status

"""There are different status of the permit where issued permits take about 76.63%.

**Permit Type**
"""

table_permit_type = permit_inspection_data_cleaned.stb.freq(['Permit Type'])[['Permit Type','count','percent']]
table_permit_type

"""**Inspection type**"""

table_inspection_type = permit_inspection_data_cleaned.stb.freq(['Inspection Type'])[['Inspection Type','count','percent']]
table_inspection_type

"""### 4.1 Visualization

Here we will just visualize the number of applications corresponding to different permit types.
"""

import plotly.express as px

fig = px.bar(table_permit_type, x='Permit Type', y='count', text_auto='.0f',width=800, height=500, template = 'plotly_white')
fig.update_layout(
  margin=dict(l=20, r=20, t=40, b=20),
  xaxis=dict(
    title_text="Permit Type",
    tickmode='linear',
    titlefont=dict(size=15),
  ),
title="Number of applications by permit types."
)

"""As we see that the most applications were submitted to request permits for building repair among all permit types. There are only 54 permits issued for building relocation, and 62 permits for nonbuilding demolition. Now, lets make a comparision between permits for building repairs and not for building repairs."""

# create a boolean column based on permit type
table_permit_type["Building Repair"] = table_permit_type["Permit Type"] == "Bldg-Alter/Repair"

# grouping and counting
result = table_permit_type.groupby("Building Repair")[["count", "percent"]].sum()
print(result)

"""After grouping the information, we see building repair only accounts for 23.93% of all permits. Remaining 76.07% of permits are distributed to another 16 permit types.

## **5. Question 2**

**Make a table and a visualization showing the number of inspections by geography. In a sentence or two describe any patterns you observe.**
"""

permit_inspection_data_cleaned.head()

table_contractor_city = permit_inspection_data_cleaned.stb.freq(['Contractor City'])[['Contractor City', 'count']]
table_contractor_city

"""As we see that, there are many spcecial characters in contractor city, it makes the result is not accurate. We will try to remove all these special characters."""

permit_inspection_data_cleaned['Contractor City'] = permit_inspection_data_cleaned['Contractor City'].str.replace(r'[^a-zA-Z]', '', regex=True)

table_contractor_city = permit_inspection_data_cleaned.stb.freq(['Contractor City'])[['Contractor City', 'count', 'percent']]
table_contractor_city

# using pie chart
fig2 = px.pie(table_contractor_city, values='count', names='Contractor City',
              title="Number of inspections by geography")
# remove all labels inside the pie chart
fig2.update_traces(textinfo='none')

"""Since we use the data for California and there are 869 cities/areas, its hard to visualize using pie chart. But, we can see that the significant number of inspections is from Los Angeles, which accounts for about 16% of applications."""

permit_inspection_data_cleaned["Contractor City"] = permit_inspection_data_cleaned["Contractor City"].astype(str)

# create 'Area' column to classify as 'LA' or 'Non-LA'
permit_inspection_data_cleaned["Area"] = np.where(permit_inspection_data_cleaned["Contractor City"].str.upper() == "LOSANGELES", "LA", "Non-LA")

# group by 'Area' and count
tab1 = permit_inspection_data_cleaned.stb.freq(['Area'])[['Area', 'count', 'percent']]
tab1

"""**Make a table and a visualization showing the results of inspections across geographies. In a sentence or two describe any patterns you observe.**


"""

tab3 = permit_inspection_data_cleaned.stb.freq(['Area', 'Inspection Result'])[['Area', 'Inspection Result', 'count', 'percent']]
tab3

tab3['percent'] = tab3['percent'].astype(float)

fig3 = px.bar(
    tab3,
    x="Area",
    y="percent",
    color="Inspection Result",
    barmode="group",
    labels={
        "percent": "Proportion of Inspections (%)",
        "Area": "Geographical Areas"
    },
    height=600,
    text=tab3["percent"].apply(lambda x: f'{x:.2f}%'),
    title="Proportion of different inspections results in LA versus non-LA urban areas."
)

fig3.show()

"""From this graph, we can see that the total inspection applications receive results are more in non-LA area in compared to in LA. Among all inspection results, approved inspections account for the highest percentage in both LA and non-LA areas.

**Were there any permits that did not get an inspection?**
"""

not_inspected_list = list(set(permit_data['PCIS Permit #'].to_list()) - set(inspections_data['PERMIT'].to_list()))
print("Number of permits that did not get inspection: ",len(not_inspected_list))

"""## **6. Question 3**

**Your manager is convinced 'out of town' contractors are not as invested in the success of their projects and so are the main culprits when it comes to violations. You are asked to complete an analysis to test this hypothesis.**

**Produce a model that quantifies the relationship between a contractor's place of origin and their inspection outcome history. Investigate any other relevant factors as necessary.**

**Interpret your results and produce a clear response for your manager.**
"""

permit_inspection_data_cleaned.head()

"""Let's make a subset of features considered in this model."""

model_data = permit_inspection_data_cleaned.loc[:,['Area','Permit Type','Inspection Type','Inspection Result']]

print(model_data.head)

"""For inspection result, we will binarize it into violation and non-violation."""

# create a new vairable
model_data["outcome"] = np.where(model_data["Inspection Result"].str.upper() == "VIOLATION OBSERVED", "Yes", "No")

outcome_by_area = model_data.groupby('Area')['outcome'].value_counts().unstack(fill_value=0)
print(outcome_by_area)

model_data.describe()

"""Since we don't have numerical or continuous variables, we will not check correlation plot."""

model_data.rename(columns={'Permit Type': 'Permit_Type', 'Inspection Type': 'Inspection_Type'}, inplace=True)

# drop the 'Inspection_Result' column
model_data = model_data.drop(columns=['Inspection Result'])

model_data.head()

import pandas as pd
import statsmodels.api as sm

# convert categorical variables to the appropriate type
model_data['Permit_Type'] = model_data['Permit_Type'].astype('category')
model_data['Inspection_Type'] = model_data['Inspection_Type'].astype('category')
model_data['Area'] = model_data['Area'].astype('category')
model_data['outcome'] = model_data['outcome'].astype('category')

# Fit the logistic regression model using GLM
stat_model = sm.formula.glm(
    formula='outcome ~ Permit_Type + Inspection_Type + C(Area)',
    data=model_data,
    family=sm.families.Binomial()
)

# Fit the model and get the results
results = stat_model.fit()

print(results.summary())